# AI Research — KITZ Knowledge Base Intelligence

> Module: AI Research | Sources: 3 | Auto-generated from KITZ Knowledge Base

---


## Safety Framework


### AI Safety Rubric (arXiv) [Medium]

- **ID:** PKB-121
- **Type:** Paper
- **URL:** https://arxiv.org/abs/2409.08751
- **Why KITZ Needs It:** Frameworks for evaluating model safety

**Extracted Intelligence:**

```
[2409.08751] A Grading Rubric for AI Safety Frameworks
Computer Science > Computers and Society
arXiv:2409.08751
(cs)
[Submitted on 13 Sep 2024]
Title:
A Grading Rubric for AI Safety Frameworks
Authors:
Jide Alaga
,
Jonas Schuett
,
Markus Anderljung
View a PDF of the paper titled A Grading Rubric for AI Safety Frameworks, by Jide Alaga and Jonas Schuett and Markus Anderljung
View PDF
HTML (experimental)
Abstract:
Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying frontier AI systems to an acceptable level. Major players like Anthropic, OpenAI, and Google DeepMind have already published their frameworks, while another 13 companies have signaled their intent to release similar frameworks by February 2025. Given their central role in AI companies' efforts to identify and address unacceptable risks from their systems, AI safety frameworks warrant significant scrutiny. To enable governments, academia, and civil society to pass judgment on these frameworks, this paper proposes a grading rubric. The rubric consists of seven evaluation criteria and 21 indicators that concretize the criteria. Each criterion can be graded on a scale from A (gold standard) to F (substandard). The paper also suggests three methods for applying the rubric: surveys, Delphi studies, and audits. The purpose of the grading rubric is to enable nuanced comparisons between frameworks, identify potential areas of improvement, and promote a race to the top in responsible AI development.
Comments:
16 pages, 4 tables
Subjects:
Computers and Society (cs.CY)
Cite as:
arXiv:2409.08751
[cs.CY]
(or
arXiv:2409.08751v1
[cs.CY]
for this version)
https://doi.org/10.48550/arXiv.2409.08751
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Jonas Schuett [
view email
]
[v1]
Fri, 13 Sep 2024 12:01:55 UTC (201 KB)
Full-text links:
Access Paper:
View a PDF of the paper titled A Grading Rubric for AI Safety Frameworks, by Jide Alaga and Jonas Schuett and Markus Anderljung
View PDF
HTML (experimental)
TeX Source
view license
Current browse context:
cs.CY
< prev
|
next >
new
|
recent
|
2024-09
Change to browse by:
cs
References & Citations
NASA ADS
Google Scholar
Semantic Scholar
export BibTeX citation
Loading...
BibTeX formatted citation
×
loading...
Data provided by:
Bookmark
Bibliographic Tools
Bibliographic and C
[...truncated]
```



## EU Compliance


### EU AI Act Practices (arXiv) [Low]

- **ID:** PKB-122
- **Type:** Paper
- **URL:** https://arxiv.org/abs/2504.15181
- **Why KITZ Needs It:** Safety governance practices of providers

**Extracted Intelligence:**

```
[2504.15181] Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures
Computer Science > Computers and Society
arXiv:2504.15181
(cs)
[Submitted on 21 Apr 2025 (
v1
), last revised 22 Jul 2025 (this version, v2)]
Title:
Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures
Authors:
Lily Stelling
,
Mick Yang
,
Rokas Gipiškis
,
Leon Staufer
,
Ze Shen Chin
,
Siméon Campos
,
Ariel Gil
,
Michael Chen
View a PDF of the paper titled Mapping Industry Practices to the EU AI Act's GPAI Code of Practice Safety and Security Measures, by Lily Stelling and 7 other authors
View PDF
Abstract:
This report provides a detailed comparison between the Safety and Security measures proposed in the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and the current commitments and practices voluntarily adopted by leading AI companies. As the EU moves toward enforcing binding obligations for GPAI model providers, the Code of Practice will be key for bridging legal requirements with concrete technical commitments. Our analysis focuses on the draft's Safety and Security section (Commitments II.1-II.16), documenting excerpts from current public-facing documents that are relevant to each individual measure.
We systematically reviewed different document types, such as companies' frontier safety frameworks and model cards, from over a dozen companies, including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and others. This report is not meant to be an indication of legal compliance, nor does it take any prescriptive viewpoint about the Code of Practice or companies' policies. Instead, it aims to inform the ongoing dialogue between regulators and General-Purpose AI model providers by surfacing evidence of industry precedent for various measures. Nonetheless, we were able to find relevant quotes from at least 5 companies' documents for the majority of the measures in Commitments II.1-II.16.
Comments:
166 pages, the Oxford Martin AI Governance Initiative
Subjects:
Computers and Society (cs.CY)
; Artificial Intelligence (cs.AI)
Cite as:
arXiv:2504.15181
[cs.CY]
(or
arXiv:2504.15181v2
[cs.CY]
for this version)
https://doi.org/10.48550/arXiv.2504.15181
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Rokas Gipiškis [
view email
]
[v1]
Mon, 21 Apr 2025 15:44:01 UTC (2,716 KB)
[v2]
Tue, 22 Jul 2025 19:27:15 UTC (4,000 KB)
Full-text links:
Access Paper:
View
[...truncated]
```



## Transparency


### AI Transparency Pipeline (arXiv) [Low]

- **ID:** PKB-123
- **Type:** Paper
- **URL:** https://arxiv.org/abs/2512.12443
- **Why KITZ Needs It:** Transparency scoring for model docs

**Extracted Intelligence:**

```
[2512.12443] AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline
Computer Science > Artificial Intelligence
arXiv:2512.12443
(cs)
[Submitted on 13 Dec 2025]
Title:
AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline
Authors:
Akhmadillo Mamirov
,
Faiaz Azmain
,
Hanyu Wang
View a PDF of the paper titled AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline, by Akhmadillo Mamirov and 2 other authors
View PDF
HTML (experimental)
Abstract:
AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.
Subjects:
Artificial Intelligence (cs.AI)
; Software Engineering (cs.SE)
Cite as:
arXiv:2512.12443
[cs.AI]
(or
arXiv:2512.12443v1
[cs.AI]
for this version)
https://doi.org/10.48550/arXiv.2512.12443
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Akhmadillo Mamirov [
view email
]
[v1]
Sat, 13 Dec 2025 19:48:44 UTC (737 KB)
Full-text links:
Access Paper:
View a PDF of the paper titled AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline, by Akhmadillo Mamirov and 2 other authors
View PDF
HTML (experime
[...truncated]
```

